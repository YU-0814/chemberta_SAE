{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92c3c6b3",
   "metadata": {},
   "source": [
    "# ChemBERTa (reproduce)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4aac0fa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROOT: /home/yoo122333/capstone/chemberta_repro_final\n",
      "CODE_ROOT: /home/yoo122333/capstone/chemberta_repro_final/code/bert-loves-chemistry\n",
      "RUNS_DIR: /home/yoo122333/capstone/chemberta_repro_final/runs\n",
      "DATA_DIR: /home/yoo122333/capstone/chemberta_repro_final/code/bert-loves-chemistry/chemberta/data\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import os, sys, json\n",
    "\n",
    "# 프로젝트 루트 (필요 시 수정)\n",
    "ROOT = Path('/home/yoo122333/capstone/chemberta_repro_final')\n",
    "CODE_ROOT = ROOT / 'code' / 'bert-loves-chemistry'\n",
    "RUNS_DIR = ROOT / 'runs'\n",
    "DATA_DIR = CODE_ROOT / 'chemberta' / 'data'\n",
    "\n",
    "# 환경 변수\n",
    "os.environ.setdefault('CUDA_VISIBLE_DEVICES', '5') # cuda 디바이스 설정\n",
    "os.environ.setdefault('TOKENIZERS_PARALLELISM', 'false') # 토크나이저 병렬처리 비활성화\n",
    "os.environ.setdefault('WANDB_DISABLED', 'true') # Weights & Biases 비활성화\n",
    "\n",
    "# PYTHONPATH 세팅\n",
    "sys.path.insert(0, str(CODE_ROOT))\n",
    "\n",
    "print('ROOT:', ROOT)\n",
    "print('CODE_ROOT:', CODE_ROOT)\n",
    "print('RUNS_DIR:', RUNS_DIR)\n",
    "print('DATA_DIR:', DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b31a4c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoo122333/micromamba/envs/chemberta-repro/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoo122333/micromamba/envs/chemberta-repro/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import (\n",
    "    RobertaTokenizerFast,\n",
    "    RobertaConfig,\n",
    "    DataCollatorForLanguageModeling,\n",
    ")\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c390438",
   "metadata": {},
   "source": [
    "## 0) 토크나이저 + 설정\n",
    "`seyonec/ChemBERTa-zinc-base-v1`을 기본으로 사용합니다.\n",
    "로컬 캐시에 있으면 인터넷 없이 로드됩니다.\n",
    "\n",
    "- 구조는 노트북 내부 구현을 사용합니다.\n",
    "- pretrained weights는 **state_dict만 로드**하여 주입합니다.\n",
    "\n",
    "기본 config는 **원 논문 설정**(RoBERTa base 스타일)을 사용합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b25073b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded tokenizer from local cache\n",
      "vocab size: 767\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = 'seyonec/ChemBERTa-zinc-base-v1' # 원논문에서 가중치는 huggingface에 업로드 되어 있다고 언급\n",
    "USE_PRETRAINED_WEIGHTS = True\n",
    "LOCAL_ONLY = True  # 로컬 캐시만 사용\n",
    "\n",
    "# 원 논문 기준 RoBERTa config (base 스타일)\n",
    "PAPER_CONFIG = dict(\n",
    "    vocab_size=52000,\n",
    "    max_position_embeddings=512,\n",
    "    num_hidden_layers=6,\n",
    "    num_attention_heads=12,\n",
    "    hidden_size=768,\n",
    "    intermediate_size=3072,\n",
    "    type_vocab_size=1,\n",
    "    hidden_act='gelu',\n",
    "    hidden_dropout_prob=0.1,\n",
    "    attention_probs_dropout_prob=0.1,\n",
    "    layer_norm_eps=1e-5,\n",
    "    pad_token_id=1,\n",
    ")\n",
    "\n",
    "try:\n",
    "    tokenizer = RobertaTokenizerFast.from_pretrained(MODEL_NAME, local_files_only=LOCAL_ONLY)\n",
    "    print('Loaded tokenizer from local cache')\n",
    "except Exception:\n",
    "    print('Local cache not found, downloading...')\n",
    "    tokenizer = RobertaTokenizerFast.from_pretrained(MODEL_NAME, local_files_only=False)\n",
    "\n",
    "print('vocab size:', tokenizer.vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca8462d",
   "metadata": {},
   "source": [
    "## 1) RoBERTa 구조 (노트북 내부 구현)\n",
    "아래 클래스들이 바로 수정 가능한 **직접 구현 코드**입니다.\n",
    "필요한 실험은 여기서 바로 수정하세요.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a762620d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_activation(name):\n",
    "    if name in ('gelu', 'gelu_new', 'gelu_fast'):\n",
    "        return F.gelu\n",
    "    if name == 'relu':\n",
    "        return F.relu\n",
    "    if name == 'tanh':\n",
    "        return torch.tanh\n",
    "    raise ValueError(name)\n",
    "\n",
    "class RobertaEmbeddings(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=config.pad_token_id)\n",
    "        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size, padding_idx=config.pad_token_id)\n",
    "        self.token_type_embeddings = nn.Embedding(config.type_vocab_size, config.hidden_size)\n",
    "        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.pad_token_id = config.pad_token_id\n",
    "\n",
    "    def create_position_ids_from_input_ids(self, input_ids):\n",
    "        mask = input_ids.ne(self.pad_token_id).int()\n",
    "        incremental_indices = (torch.cumsum(mask, dim=1).type_as(mask)) * mask\n",
    "        return incremental_indices.long() + self.pad_token_id\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids=None):\n",
    "        if token_type_ids is None:\n",
    "            token_type_ids = torch.zeros_like(input_ids)\n",
    "        position_ids = self.create_position_ids_from_input_ids(input_ids)\n",
    "        embeddings = self.word_embeddings(input_ids)\n",
    "        embeddings = embeddings + self.position_embeddings(position_ids)\n",
    "        embeddings = embeddings + self.token_type_embeddings(token_type_ids)\n",
    "        embeddings = self.LayerNorm(embeddings)\n",
    "        embeddings = self.dropout(embeddings)\n",
    "        return embeddings\n",
    "\n",
    "class RobertaSelfAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.num_heads = config.num_attention_heads\n",
    "        self.head_dim = config.hidden_size // config.num_attention_heads\n",
    "        self.all_head_size = self.num_heads * self.head_dim\n",
    "\n",
    "        self.query = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "        self.key = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "        self.value = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n",
    "\n",
    "    def transpose_for_scores(self, x):\n",
    "        new_x_shape = x.size()[:-1] + (self.num_heads, self.head_dim)\n",
    "        x = x.view(*new_x_shape)\n",
    "        return x.permute(0, 2, 1, 3)\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask=None):\n",
    "        query_layer = self.transpose_for_scores(self.query(hidden_states))\n",
    "        key_layer = self.transpose_for_scores(self.key(hidden_states))\n",
    "        value_layer = self.transpose_for_scores(self.value(hidden_states))\n",
    "\n",
    "        attn_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
    "        attn_scores = attn_scores / math.sqrt(self.head_dim)\n",
    "        if attention_mask is not None:\n",
    "            attn_scores = attn_scores + attention_mask\n",
    "        attn_probs = F.softmax(attn_scores, dim=-1)\n",
    "        attn_probs = self.dropout(attn_probs)\n",
    "        context = torch.matmul(attn_probs, value_layer)\n",
    "        context = context.permute(0, 2, 1, 3).contiguous()\n",
    "        new_context_shape = context.size()[:-2] + (self.all_head_size,)\n",
    "        context = context.view(*new_context_shape)\n",
    "        return context, attn_probs\n",
    "\n",
    "class RobertaSelfOutput(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "    def forward(self, hidden_states, input_tensor):\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
    "        return hidden_states\n",
    "\n",
    "class RobertaAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.self = RobertaSelfAttention(config)\n",
    "        self.output = RobertaSelfOutput(config)\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask=None):\n",
    "        self_outputs = self.self(hidden_states, attention_mask=attention_mask)\n",
    "        attn_output = self.output(self_outputs[0], hidden_states)\n",
    "        return attn_output, self_outputs[1]\n",
    "\n",
    "class RobertaIntermediate(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)\n",
    "        self.act = get_activation(config.hidden_act)\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        return self.act(self.dense(hidden_states))\n",
    "\n",
    "class RobertaOutput(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(config.intermediate_size, config.hidden_size)\n",
    "        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "    def forward(self, hidden_states, input_tensor):\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
    "        return hidden_states\n",
    "\n",
    "class RobertaLayer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.attention = RobertaAttention(config)\n",
    "        self.intermediate = RobertaIntermediate(config)\n",
    "        self.output = RobertaOutput(config)\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask=None):\n",
    "        attn_output, _ = self.attention(hidden_states, attention_mask=attention_mask)\n",
    "        intermediate_output = self.intermediate(attn_output)\n",
    "        layer_output = self.output(intermediate_output, attn_output)\n",
    "        return layer_output\n",
    "\n",
    "class RobertaEncoder(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.layer = nn.ModuleList([RobertaLayer(config) for _ in range(config.num_hidden_layers)])\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask=None):\n",
    "        for layer_module in self.layer:\n",
    "            hidden_states = layer_module(hidden_states, attention_mask=attention_mask)\n",
    "        return hidden_states\n",
    "\n",
    "class RobertaModel(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.embeddings = RobertaEmbeddings(config)\n",
    "        self.encoder = RobertaEncoder(config)\n",
    "        self.config = config\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, token_type_ids=None):\n",
    "        if attention_mask is None:\n",
    "            attention_mask = torch.ones_like(input_ids)\n",
    "        extended_mask = (1.0 - attention_mask[:, None, None, :]) * -10000.0\n",
    "        embeddings = self.embeddings(input_ids, token_type_ids=token_type_ids)\n",
    "        sequence_output = self.encoder(embeddings, attention_mask=extended_mask)\n",
    "        return sequence_output\n",
    "\n",
    "class RobertaLMHead(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.layer_norm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        self.decoder = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n",
    "        self.bias = nn.Parameter(torch.zeros(config.vocab_size))\n",
    "        self.decoder.bias = self.bias\n",
    "\n",
    "    def forward(self, features):\n",
    "        x = self.dense(features)\n",
    "        x = F.gelu(x)\n",
    "        x = self.layer_norm(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "class RobertaForMaskedLM(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.roberta = RobertaModel(config)\n",
    "        self.lm_head = RobertaLMHead(config)\n",
    "        self.config = config\n",
    "\n",
    "    def tie_weights(self):\n",
    "        self.lm_head.decoder.weight = self.roberta.embeddings.word_embeddings.weight\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, labels=None):\n",
    "        sequence_output = self.roberta(input_ids, attention_mask=attention_mask)\n",
    "        logits = self.lm_head(sequence_output)\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fn = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "            loss = loss_fn(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
    "        return logits, loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b822dbb0",
   "metadata": {},
   "source": [
    "## 2) Pretrained weights 로드 (모델 클래스 없이 state_dict만)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "205a987d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_state_dict_from_hf(model_name, local_only=True): # 허깅페이스에서 학습된 가중치를 가져오는 함수\n",
    "    snapshot_dir = snapshot_download(repo_id=model_name, local_files_only=local_only)\n",
    "    safetensors_path = Path(snapshot_dir) / 'model.safetensors'\n",
    "    bin_path = Path(snapshot_dir) / 'pytorch_model.bin'\n",
    "    if safetensors_path.exists():\n",
    "        from safetensors.torch import load_file\n",
    "        return load_file(str(safetensors_path))\n",
    "    return torch.load(str(bin_path), map_location='cpu')\n",
    "\n",
    "def load_config_from_hf(model_name, local_only=True):\n",
    "    snapshot_dir = snapshot_download(repo_id=model_name, local_files_only=local_only)\n",
    "    cfg_path = Path(snapshot_dir) / 'config.json'\n",
    "    with open(cfg_path) as f:\n",
    "        cfg_dict = json.load(f)\n",
    "    return RobertaConfig(**cfg_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "356bda50",
   "metadata": {},
   "source": [
    "## 3) MLM 파이프라인 (ZINC)\n",
    "- 데이터 로드\n",
    "- Dataset/Dataloader\n",
    "- 모델 로드 (pretrained weights 주입 가능)\n",
    "- 학습/평가 루프\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7168dd8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLM dataset path: /home/yoo122333/capstone/chemberta_repro_final/code/bert-loves-chemistry/chemberta/data/100k_rndm_zinc_drugs_clean.txt\n",
      "MLM samples: 100000\n"
     ]
    }
   ],
   "source": [
    "# ZINC SMILES 텍스트 파일 토큰화해서 15퍼센트 마스킹 랜덤 마스킹하고 배치 생성하는 파이프라인\n",
    "\n",
    "class MLMSmilesDataset(Dataset): # smiles 문자열을 mlm 학습용으로 변환하는 커스텀 데이터셋\n",
    "    def __init__(self, path, tokenizer, max_len=128):\n",
    "        self.lines = [l.strip() for l in open(path) if l.strip()]\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.lines)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        enc = self.tokenizer(\n",
    "            self.lines[idx],\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_len,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        return {k: v.squeeze(0) for k, v in enc.items()}\n",
    "\n",
    "# ZINC 데이터 우선 사용\n",
    "zinc_candidates = [\n",
    "    DATA_DIR / '100k_rndm_zinc_drugs_clean.txt',\n",
    "    DATA_DIR / '250k_rndm_zinc_drugs_clean_sorted copy.txt',\n",
    "]\n",
    "mlm_path = None\n",
    "for p in zinc_candidates:\n",
    "    if p.exists():\n",
    "        mlm_path = p\n",
    "        break\n",
    "if mlm_path is None:\n",
    "    mlm_path = DATA_DIR / 'pubchem_1k_smiles.txt'\n",
    "\n",
    "print('MLM dataset path:', mlm_path)\n",
    "mlm_dataset = MLMSmilesDataset(mlm_path, tokenizer, max_len=128)\n",
    "mlm_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True, mlm_probability=0.15)\n",
    "mlm_loader = DataLoader(mlm_dataset, batch_size=8, shuffle=True, collate_fn=mlm_collator)\n",
    "\n",
    "print('MLM samples:', len(mlm_dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b704209b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1684543/2236783688.py:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(str(bin_path), map_location='cpu')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1684543/2236783688.py:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(str(bin_path), map_location='cpu')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "missing keys: 0\n",
      "unexpected keys: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1684543/2236783688.py:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(str(bin_path), map_location='cpu')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "missing keys: 0\n",
      "unexpected keys: 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RobertaForMaskedLM(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(767, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (lm_head): RobertaLMHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (decoder): Linear(in_features=768, out_features=767, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MLM 모델 준비 (원 논문 config 우선)\n",
    "if USE_PRETRAINED_WEIGHTS:\n",
    "    # pretrained weights는 HF state_dict에서만 로드\n",
    "    state_dict = load_state_dict_from_hf(MODEL_NAME, local_only=LOCAL_ONLY)\n",
    "    # weights와 config mismatch를 피하려면 HF config를 사용\n",
    "    config = load_config_from_hf(MODEL_NAME, local_only=LOCAL_ONLY)\n",
    "else:\n",
    "    cfg = dict(PAPER_CONFIG)\n",
    "    # 토크나이저 vocab에 맞춤\n",
    "    cfg['vocab_size'] = tokenizer.vocab_size\n",
    "    config = RobertaConfig(**cfg)\n",
    "    state_dict = None\n",
    "\n",
    "mlm_model = RobertaForMaskedLM(config)\n",
    "mlm_model.tie_weights()\n",
    "if state_dict is not None:\n",
    "    missing, unexpected = mlm_model.load_state_dict(state_dict, strict=False)\n",
    "    print('missing keys:', len(missing))\n",
    "    print('unexpected keys:', len(unexpected))\n",
    "\n",
    "mlm_model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "875ad9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLM 학습/평가 루프\n",
    "from torch.optim import AdamW\n",
    "\n",
    "def train_mlm_epoch(model, loader, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for batch in loader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        logits, loss = model(**batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / max(1, len(loader))\n",
    "\n",
    "def eval_mlm_epoch(model, loader):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            logits, loss = model(**batch)\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / max(1, len(loader))\n",
    "\n",
    "optimizer = AdamW(mlm_model.parameters(), lr=5e-5)\n",
    "\n",
    "# for epoch in range(2):\n",
    "#     train_loss = train_mlm_epoch(mlm_model, mlm_loader, optimizer)\n",
    "#     eval_loss = eval_mlm_epoch(mlm_model, mlm_loader)\n",
    "#     print(f'[MLM] epoch {epoch+1} | train_loss={train_loss:.4f} | eval_loss={eval_loss:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493a346b",
   "metadata": {},
   "source": [
    "## 4) Regression 파이프라인\n",
    "- 데이터 로드\n",
    "- Dataset/Dataloader\n",
    "- 모델 로드 (pretrained weights 주입 가능)\n",
    "- 학습/평가 루프\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7791877d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regression samples: 998\n"
     ]
    }
   ],
   "source": [
    "class SmilesRegressionDataset(Dataset):\n",
    "    def __init__(self, csv_path, tokenizer, max_len=128, norm_path=None):\n",
    "        df = pd.read_csv(csv_path)\n",
    "        self.smiles = df.iloc[:, 0].astype(str).tolist()\n",
    "        self.labels = df.iloc[:, 1:].astype(float).values\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "        if norm_path:\n",
    "            with open(norm_path) as f:\n",
    "                norm = json.load(f)\n",
    "            mean = torch.tensor(norm['mean'], dtype=torch.float32)\n",
    "            std = torch.tensor(norm['std'], dtype=torch.float32)\n",
    "            std = torch.where(std == 0, torch.ones_like(std), std)\n",
    "            self.labels = (torch.tensor(self.labels, dtype=torch.float32) - mean) / std\n",
    "            self.labels = self.labels.numpy()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.smiles)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        enc = self.tokenizer(\n",
    "            self.smiles[idx],\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_len,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        item = {k: v.squeeze(0) for k, v in enc.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.float32)\n",
    "        return item\n",
    "\n",
    "reg_csv = DATA_DIR / 'pubchem_descriptors_sample_1k_clean.csv'\n",
    "reg_norm = DATA_DIR / 'pubchem_descriptors_sample_1k_normalization_values_199.json'\n",
    "\n",
    "reg_dataset = SmilesRegressionDataset(reg_csv, tokenizer, max_len=128, norm_path=reg_norm)\n",
    "reg_loader = DataLoader(reg_dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "print('Regression samples:', len(reg_dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ac519eea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1684543/2236783688.py:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(str(bin_path), map_location='cpu')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1684543/2236783688.py:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(str(bin_path), map_location='cpu')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "missing keys: 4\n",
      "unexpected keys: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1684543/2236783688.py:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(str(bin_path), map_location='cpu')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "missing keys: 4\n",
      "unexpected keys: 9\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RobertaForRegression(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(767, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (regression): RobertaRegressionHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=199, bias=True)\n",
       "  )\n",
       "  (loss_fn): MSELoss()\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class RobertaRegressionHead(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.out_proj = nn.Linear(config.hidden_size, config.num_labels)\n",
    "\n",
    "    def forward(self, features):\n",
    "        x = features[:, 0, :]\n",
    "        x = self.dropout(x)\n",
    "        x = self.dense(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.out_proj(x)\n",
    "        return x\n",
    "\n",
    "class RobertaForRegression(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.roberta = RobertaModel(config)\n",
    "        self.regression = RobertaRegressionHead(config)\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, labels=None):\n",
    "        sequence_output = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = self.regression(sequence_output)\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss = self.loss_fn(logits, labels)\n",
    "        return logits, loss\n",
    "\n",
    "# Regression 모델 준비\n",
    "if USE_PRETRAINED_WEIGHTS:\n",
    "    config = load_config_from_hf(MODEL_NAME, local_only=LOCAL_ONLY)\n",
    "    config.num_labels = reg_dataset[0]['labels'].numel()\n",
    "    state_dict = load_state_dict_from_hf(MODEL_NAME, local_only=LOCAL_ONLY)\n",
    "else:\n",
    "    cfg = dict(PAPER_CONFIG)\n",
    "    cfg['vocab_size'] = tokenizer.vocab_size\n",
    "    cfg['num_labels'] = reg_dataset[0]['labels'].numel()\n",
    "    config = RobertaConfig(**cfg)\n",
    "    state_dict = None\n",
    "\n",
    "reg_model = RobertaForRegression(config)\n",
    "if state_dict is not None:\n",
    "    missing, unexpected = reg_model.load_state_dict(state_dict, strict=False)\n",
    "    print('missing keys:', len(missing))\n",
    "    print('unexpected keys:', len(unexpected))\n",
    "\n",
    "reg_model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "651fe407",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "\n",
    "def train_reg_epoch(model, loader, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for batch in loader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        logits, loss = model(**batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / max(1, len(loader))\n",
    "\n",
    "def eval_reg_epoch(model, loader):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            logits, loss = model(**batch)\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / max(1, len(loader))\n",
    "\n",
    "optimizer = AdamW(reg_model.parameters(), lr=5e-5)\n",
    "\n",
    "# for epoch in range(2):\n",
    "#     train_loss = train_reg_epoch(reg_model, reg_loader, optimizer)\n",
    "#     eval_loss = eval_reg_epoch(reg_model, reg_loader)\n",
    "#     print(f'[REG] epoch {epoch+1} | train_loss={train_loss:.4f} | eval_loss={eval_loss:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58420cb2",
   "metadata": {},
   "source": [
    "## 5) Classification 파이프라인 (MolNet, multi-task 지원)\n",
    "- MolNet(DeepChem) 로딩만 사용합니다.\n",
    "- `DATASET_NAME`만 바꿔서 실험합니다. 예: `\"bbbp\"`, `\"bace_classification\"`, `\"hiv\"`, `\"clintox\"`.\n",
    "- multi-task 데이터셋은 BCEWithLogits + 마스킹으로 처리합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a24c0d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "import numpy as np\n",
    "from torch.optim import AdamW\n",
    "\n",
    "# MoleculeNet 벤치마크 표준 스플릿 설정 (논문 Table 기준)\n",
    "MOLNET_SPLITS = {\n",
    "    \"bace_classification\": \"scaffold\",\n",
    "    \"bbbp\": \"scaffold\",\n",
    "    \"clintox\": \"scaffold\",\n",
    "}\n",
    "\n",
    "# Dataset configuration\n",
    "DATASET_NAME = \"clintox\"\n",
    "NUM_SEEDS = 5  # ChemBERTa paper uses 5 seeds (0-4) for reporting mean±std\n",
    "\n",
    "CLS_MAX_LEN = 128\n",
    "CLS_BATCH_SIZE = 64  # ChemBERTa paper uses 64\n",
    "CLS_EPOCHS = 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c78c4141",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'split' is deprecated.  Use 'splitter' instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: clintox | Split type: scaffold\n",
      "Using tasks ['CT_TOX'] from available tasks for clintox: ['FDA_APPROVED', 'CT_TOX']\n",
      "train/valid/test: 1184 148 148\n",
      "tasks: ['CT_TOX']\n"
     ]
    }
   ],
   "source": [
    "from chemberta.utils.molnet_dataloader import load_molnet_dataset\n",
    "\n",
    "def _standardize_molnet_df(df, label_cols):\n",
    "    df = df.copy()\n",
    "    if \"smiles\" not in df.columns:\n",
    "        df = df.rename(columns={df.columns[0]: \"smiles\"})\n",
    "    cols = [\"smiles\"] + label_cols\n",
    "    df = df[cols]\n",
    "    for c in label_cols:\n",
    "        df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "    df = df[df[label_cols].notna().any(axis=1)]\n",
    "    return df\n",
    "\n",
    "# MoleculeNet 벤치마크 표준에 따른 스플릿 선택\n",
    "split_type = MOLNET_SPLITS.get(DATASET_NAME.lower(), \"scaffold\")\n",
    "print(f\"Dataset: {DATASET_NAME} | Split type: {split_type}\")\n",
    "\n",
    "# Load dataset using DeepChem\n",
    "tasks, (train_df, valid_df, test_df), _ = load_molnet_dataset(\n",
    "    DATASET_NAME, split=split_type, df_format=\"chemprop\"\n",
    ")\n",
    "label_cols = list(tasks)\n",
    "\n",
    "train_df = _standardize_molnet_df(train_df, label_cols)\n",
    "valid_df = _standardize_molnet_df(valid_df, label_cols)\n",
    "test_df = _standardize_molnet_df(test_df, label_cols)\n",
    "\n",
    "NUM_TASKS = len(label_cols)\n",
    "print(\"train/valid/test:\", len(train_df), len(valid_df), len(test_df))\n",
    "print(\"tasks:\", label_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bb3fce0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmilesClassificationDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, label_cols, max_len=128):\n",
    "        self.smiles = df[\"smiles\"].astype(str).tolist()\n",
    "        labels = df[label_cols].to_numpy(dtype=np.float32)\n",
    "        self.label_mask = ~np.isnan(labels)\n",
    "        labels = np.nan_to_num(labels, nan=0.0)\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.smiles)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        enc = self.tokenizer(\n",
    "            self.smiles[idx],\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_len,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        item = {k: v.squeeze(0) for k, v in enc.items()}\n",
    "        item[\"labels\"] = torch.tensor(self.labels[idx], dtype=torch.float32)\n",
    "        item[\"label_mask\"] = torch.tensor(self.label_mask[idx], dtype=torch.float32)\n",
    "        return item\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "86b2ab72",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RobertaClassificationHead(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.out_proj = nn.Linear(config.hidden_size, config.num_labels)\n",
    "\n",
    "    def forward(self, features):\n",
    "        x = features[:, 0, :]\n",
    "        x = self.dropout(x)\n",
    "        x = self.dense(x)\n",
    "        x = torch.tanh(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.out_proj(x)\n",
    "        return x\n",
    "\n",
    "class RobertaForSequenceClassification(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.roberta = RobertaModel(config)\n",
    "        self.classifier = RobertaClassificationHead(config)\n",
    "        self.loss_fn = nn.BCEWithLogitsLoss(reduction=\"none\")\n",
    "        self.config = config\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, labels=None, label_mask=None):\n",
    "        sequence_output = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = self.classifier(sequence_output)\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss = self.loss_fn(logits, labels)\n",
    "            if label_mask is not None:\n",
    "                loss = loss * label_mask\n",
    "                denom = label_mask.sum().clamp(min=1.0)\n",
    "                loss = loss.sum() / denom\n",
    "            else:\n",
    "                loss = loss.mean()\n",
    "        return logits, loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "19da0e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_cls_epoch(model, loader, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for batch in loader:\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "        label_mask = batch[\"label_mask\"].to(device)\n",
    "        inputs = {k: v.to(device) for k, v in batch.items() if k not in (\"labels\", \"label_mask\")}\n",
    "        logits, loss = model(**inputs, labels=labels, label_mask=label_mask)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / max(1, len(loader))\n",
    "\n",
    "def _safe_mean(vals):\n",
    "    vals = [v for v in vals if not np.isnan(v)]\n",
    "    return float(np.mean(vals)) if vals else float(\"nan\")\n",
    "\n",
    "def eval_cls_epoch(model, loader, num_tasks):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    y_true, y_prob, y_mask = [], [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            label_mask = batch[\"label_mask\"].to(device)\n",
    "            inputs = {k: v.to(device) for k, v in batch.items() if k not in (\"labels\", \"label_mask\")}\n",
    "            logits, loss = model(**inputs, labels=labels, label_mask=label_mask)\n",
    "            total_loss += loss.item()\n",
    "            probs = torch.sigmoid(logits)\n",
    "            y_true.append(labels.detach().cpu())\n",
    "            y_prob.append(probs.detach().cpu())\n",
    "            y_mask.append(label_mask.detach().cpu())\n",
    "\n",
    "    if len(y_true) == 0:\n",
    "        return 0.0, {\"roc_auc\": float(\"nan\"), \"avg_precision\": float(\"nan\")}\n",
    "\n",
    "    y_true = torch.cat(y_true).numpy()\n",
    "    y_prob = torch.cat(y_prob).numpy()\n",
    "    y_mask = torch.cat(y_mask).numpy()\n",
    "\n",
    "    roc_aucs = []\n",
    "    aps = []\n",
    "    for t in range(num_tasks):\n",
    "        mask = y_mask[:, t].astype(bool)\n",
    "        if mask.sum() == 0:\n",
    "            roc_aucs.append(float(\"nan\"))\n",
    "            aps.append(float(\"nan\"))\n",
    "            continue\n",
    "        yt = y_true[mask, t]\n",
    "        yp = y_prob[mask, t]\n",
    "        if len(np.unique(yt)) < 2:\n",
    "            roc_aucs.append(float(\"nan\"))\n",
    "            aps.append(float(\"nan\"))\n",
    "        else:\n",
    "            roc_aucs.append(roc_auc_score(yt, yp))\n",
    "            aps.append(average_precision_score(yt, yp))\n",
    "\n",
    "    return total_loss / max(1, len(loader)), {\n",
    "        \"roc_auc\": _safe_mean(roc_aucs),\n",
    "        \"avg_precision\": _safe_mean(aps),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "21397bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = SmilesClassificationDataset(train_df, tokenizer, label_cols, max_len=CLS_MAX_LEN)\n",
    "valid_dataset = SmilesClassificationDataset(valid_df, tokenizer, label_cols, max_len=CLS_MAX_LEN)\n",
    "test_dataset = SmilesClassificationDataset(test_df, tokenizer, label_cols, max_len=CLS_MAX_LEN)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=CLS_BATCH_SIZE, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=CLS_BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=CLS_BATCH_SIZE, shuffle=False)\n",
    "\n",
    "config = load_config_from_hf(MODEL_NAME, local_only=LOCAL_ONLY)\n",
    "config.num_labels = int(NUM_TASKS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1c9336a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "SEED 1/5 (seed=42)\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1684543/2236783688.py:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(str(bin_path), map_location='cpu')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] epoch 1 | train_loss=0.2721 | val_loss=0.1791 | roc_auc=0.7523 | ap=0.9872\n",
      "[CLS] epoch 2 | train_loss=0.1946 | val_loss=0.1724 | roc_auc=0.8040 | ap=0.9905\n",
      "[CLS] epoch 3 | train_loss=0.1467 | val_loss=0.1573 | roc_auc=0.8850 | ap=0.9948\n",
      "[CLS] epoch 4 | train_loss=0.0998 | val_loss=0.1682 | roc_auc=0.8815 | ap=0.9946\n",
      "[CLS] epoch 5 | train_loss=0.0739 | val_loss=0.2025 | roc_auc=0.9108 | ap=0.9961\n",
      "[CLS] test_loss=0.1745 | roc_auc=0.8841 | ap=0.9915\n",
      "============================================================\n",
      "SEED 2/5 (seed=123)\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1684543/2236783688.py:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(str(bin_path), map_location='cpu')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] epoch 1 | train_loss=0.2809 | val_loss=0.1745 | roc_auc=0.7195 | ap=0.9847\n",
      "[CLS] epoch 2 | train_loss=0.1979 | val_loss=0.1654 | roc_auc=0.7653 | ap=0.9876\n",
      "[CLS] epoch 3 | train_loss=0.1383 | val_loss=0.1517 | roc_auc=0.9120 | ap=0.9962\n",
      "[CLS] epoch 4 | train_loss=0.0902 | val_loss=0.1687 | roc_auc=0.8873 | ap=0.9947\n",
      "[CLS] epoch 5 | train_loss=0.0645 | val_loss=0.1650 | roc_auc=0.9378 | ap=0.9973\n",
      "[CLS] test_loss=0.1939 | roc_auc=0.8881 | ap=0.9914\n",
      "============================================================\n",
      "SEED 3/5 (seed=1337)\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1684543/2236783688.py:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(str(bin_path), map_location='cpu')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] epoch 1 | train_loss=0.3206 | val_loss=0.1914 | roc_auc=0.6174 | ap=0.9694\n",
      "[CLS] epoch 2 | train_loss=0.2190 | val_loss=0.2012 | roc_auc=0.6796 | ap=0.9794\n",
      "[CLS] epoch 3 | train_loss=0.1629 | val_loss=0.1664 | roc_auc=0.7864 | ap=0.9879\n",
      "[CLS] epoch 4 | train_loss=0.1140 | val_loss=0.1873 | roc_auc=0.8404 | ap=0.9920\n",
      "[CLS] epoch 5 | train_loss=0.0855 | val_loss=0.1943 | roc_auc=0.9143 | ap=0.9962\n",
      "[CLS] test_loss=0.1911 | roc_auc=0.8273 | ap=0.9864\n",
      "============================================================\n",
      "SEED 4/5 (seed=2023)\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1684543/2236783688.py:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(str(bin_path), map_location='cpu')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] epoch 1 | train_loss=0.2829 | val_loss=0.2131 | roc_auc=0.7265 | ap=0.9845\n",
      "[CLS] epoch 2 | train_loss=0.1975 | val_loss=0.1590 | roc_auc=0.8392 | ap=0.9923\n",
      "[CLS] epoch 3 | train_loss=0.1511 | val_loss=0.1553 | roc_auc=0.8908 | ap=0.9951\n",
      "[CLS] epoch 4 | train_loss=0.1116 | val_loss=0.1724 | roc_auc=0.9002 | ap=0.9955\n",
      "[CLS] epoch 5 | train_loss=0.0755 | val_loss=0.1371 | roc_auc=0.9026 | ap=0.9957\n",
      "[CLS] test_loss=0.1665 | roc_auc=0.8633 | ap=0.9893\n",
      "============================================================\n",
      "SEED 5/5 (seed=9999)\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1684543/2236783688.py:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(str(bin_path), map_location='cpu')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] epoch 1 | train_loss=0.2918 | val_loss=0.1820 | roc_auc=0.6984 | ap=0.9804\n",
      "[CLS] epoch 2 | train_loss=0.2081 | val_loss=0.1694 | roc_auc=0.7688 | ap=0.9874\n",
      "[CLS] epoch 3 | train_loss=0.1510 | val_loss=0.1542 | roc_auc=0.8744 | ap=0.9942\n",
      "[CLS] epoch 4 | train_loss=0.1001 | val_loss=0.1818 | roc_auc=0.9155 | ap=0.9961\n",
      "[CLS] epoch 5 | train_loss=0.0773 | val_loss=0.1752 | roc_auc=0.9296 | ap=0.9969\n",
      "[CLS] test_loss=0.2053 | roc_auc=0.8153 | ap=0.9861\n",
      "============================================================\n",
      "FINAL RESULTS (mean ± std over 5 seeds)\n",
      "============================================================\n",
      "ROC-AUC: 0.8556 ± 0.0295\n",
      "Avg Precision: 0.9890 ± 0.0023\n",
      "Individual ROC-AUCs: ['0.8841', '0.8881', '0.8273', '0.8633', '0.8153']\n"
     ]
    }
   ],
   "source": [
    "# Multi-seed training following ChemBERTa paper (5 seeds)\n",
    "# Use distributed seeds to avoid correlation between consecutive seeds\n",
    "SEEDS = [42, 123, 1337, 2023, 9999]\n",
    "\n",
    "all_test_results = []\n",
    "\n",
    "for idx, seed in enumerate(SEEDS[:NUM_SEEDS]):\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"SEED {idx+1}/{NUM_SEEDS} (seed={seed})\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    # Set seed (before model initialization for reproducibility)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    # Reset model for each seed\n",
    "    cls_model = RobertaForSequenceClassification(config).to(device)\n",
    "    if USE_PRETRAINED_WEIGHTS:\n",
    "        state_dict_pretrained = load_state_dict_from_hf(MODEL_NAME, local_only=LOCAL_ONLY)\n",
    "        cls_model.load_state_dict(state_dict_pretrained, strict=False)\n",
    "\n",
    "    optimizer = AdamW(cls_model.parameters(), lr=5e-5)\n",
    "\n",
    "    for epoch in range(CLS_EPOCHS):\n",
    "        train_loss = train_cls_epoch(cls_model, train_loader, optimizer)\n",
    "        val_loss, val_metrics = eval_cls_epoch(cls_model, valid_loader, NUM_TASKS)\n",
    "        print(\n",
    "            f\"[CLS] epoch {epoch+1} | train_loss={train_loss:.4f} | val_loss={val_loss:.4f} | \"\n",
    "            f\"roc_auc={val_metrics['roc_auc']:.4f} | ap={val_metrics['avg_precision']:.4f}\"\n",
    "        )\n",
    "\n",
    "    test_loss, test_metrics = eval_cls_epoch(cls_model, test_loader, NUM_TASKS)\n",
    "    print(\n",
    "        f\"[CLS] test_loss={test_loss:.4f} | roc_auc={test_metrics['roc_auc']:.4f} | ap={test_metrics['avg_precision']:.4f}\"\n",
    "    )\n",
    "    all_test_results.append(test_metrics)\n",
    "\n",
    "# Calculate mean and std across seeds\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"FINAL RESULTS (mean ± std over {NUM_SEEDS} seeds)\")\n",
    "print(f\"{'='*60}\")\n",
    "roc_aucs = [r['roc_auc'] for r in all_test_results]\n",
    "aps = [r['avg_precision'] for r in all_test_results]\n",
    "print(f\"ROC-AUC: {np.mean(roc_aucs):.4f} ± {np.std(roc_aucs):.4f}\")\n",
    "print(f\"Avg Precision: {np.mean(aps):.4f} ± {np.std(aps):.4f}\")\n",
    "print(f\"Individual ROC-AUCs: {[f'{x:.4f}' for x in roc_aucs]}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chemberta-repro",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
