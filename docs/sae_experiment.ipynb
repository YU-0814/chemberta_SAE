{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ChemBERTa SAE Experiment\n",
        "\n",
        "This notebook runs: activation extraction → SAE training (resume) → downstream ROC-AUC.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Imports & Paths\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/yoo122333/micromamba/envs/chemberta-repro/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "No normalization for SPS. Feature removed!\n",
            "No normalization for AvgIpc. Feature removed!\n",
            "No normalization for NumAmideBonds. Feature removed!\n",
            "No normalization for NumAtomStereoCenters. Feature removed!\n",
            "No normalization for NumBridgeheadAtoms. Feature removed!\n",
            "No normalization for NumHeterocycles. Feature removed!\n",
            "No normalization for NumSpiroAtoms. Feature removed!\n",
            "No normalization for NumUnspecifiedAtomStereoCenters. Feature removed!\n",
            "No normalization for Phi. Feature removed!\n",
            "Skipped loading some Tensorflow models, missing a dependency. No module named 'tensorflow'\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m W&B installed but not logged in.  Run `wandb login` or set the WANDB_API_KEY env variable.\n",
            "Skipped loading modules with pytorch-geometric dependency, missing a dependency. No module named 'torch_geometric'\n",
            "Skipped loading modules with pytorch-geometric dependency, missing a dependency. cannot import name 'DMPNN' from 'deepchem.models.torch_models' (/home/yoo122333/micromamba/envs/chemberta-repro/lib/python3.10/site-packages/deepchem/models/torch_models/__init__.py)\n",
            "Skipped loading modules with pytorch-lightning dependency, missing a dependency. No module named 'lightning'\n",
            "Skipped loading some Jax models, missing a dependency. No module named 'jax'\n",
            "Skipped loading some PyTorch models, missing a dependency. No module named 'tensorflow'\n"
          ]
        }
      ],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "import csv\n",
        "import json\n",
        "import math\n",
        "import os\n",
        "import random\n",
        "import time\n",
        "from dataclasses import asdict, dataclass\n",
        "from pathlib import Path\n",
        "from typing import Iterable\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from huggingface_hub import snapshot_download\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from torch.utils.data import DataLoader, Dataset, IterableDataset\n",
        "from transformers import (\n",
        "    DataCollatorForLanguageModeling,\n",
        "    RobertaConfig,\n",
        "    RobertaTokenizerFast,\n",
        ")\n",
        "\n",
        "\n",
        "def _capstone_root() -> Path:\n",
        "    \"\"\"Find capstone project root directory.\"\"\"\n",
        "    here = Path.cwd()\n",
        "    for parent in (here,) + tuple(here.parents):\n",
        "        if (parent / \"chemberta_repro_final\").exists() and (parent / \"sparse_autoencoder\").exists():\n",
        "            return parent\n",
        "        if parent.name == \"chemberta_repro_final\":\n",
        "            candidate = parent.parent\n",
        "            if (candidate / \"sparse_autoencoder\").exists():\n",
        "                return candidate\n",
        "    return here\n",
        "\n",
        "\n",
        "def _ensure_modules_on_path() -> None:\n",
        "    \"\"\"Add required modules to Python path.\"\"\"\n",
        "    repo_root = _capstone_root()\n",
        "    \n",
        "    sae_root = repo_root / \"sparse_autoencoder\"\n",
        "    if sae_root.exists() and str(sae_root) not in os.sys.path:\n",
        "        os.sys.path.insert(0, str(sae_root))\n",
        "    \n",
        "    chemberta_root = repo_root / \"chemberta_repro_final\" / \"code\" / \"bert-loves-chemistry\"\n",
        "    if chemberta_root.exists() and str(chemberta_root) not in os.sys.path:\n",
        "        os.sys.path.insert(0, str(chemberta_root))\n",
        "\n",
        "\n",
        "_ensure_modules_on_path()\n",
        "\n",
        "from sparse_autoencoder.loss import autoencoder_loss\n",
        "from sparse_autoencoder.model import Autoencoder, TopK\n",
        "from chemberta.utils.molnet_dataloader import load_molnet_dataset\n",
        "\n",
        "\n",
        "def set_seed(seed: int) -> None:\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "\n",
        "def get_activation(name: str):\n",
        "    if name in (\"gelu\", \"gelu_new\", \"gelu_fast\"):\n",
        "        return F.gelu\n",
        "    if name == \"relu\":\n",
        "        return F.relu\n",
        "    if name == \"tanh\":\n",
        "        return torch.tanh\n",
        "    raise ValueError(name)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## RoBERTa (custom, attn_output support)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "class RobertaEmbeddings(nn.Module):\n",
        "    def __init__(self, config: RobertaConfig):\n",
        "        super().__init__()\n",
        "        self.word_embeddings = nn.Embedding(\n",
        "            config.vocab_size, config.hidden_size, padding_idx=config.pad_token_id\n",
        "        )\n",
        "        self.position_embeddings = nn.Embedding(\n",
        "            config.max_position_embeddings,\n",
        "            config.hidden_size,\n",
        "            padding_idx=config.pad_token_id,\n",
        "        )\n",
        "        self.token_type_embeddings = nn.Embedding(\n",
        "            config.type_vocab_size, config.hidden_size\n",
        "        )\n",
        "        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "        self.pad_token_id = config.pad_token_id\n",
        "\n",
        "    def create_position_ids_from_input_ids(self, input_ids: torch.Tensor) -> torch.Tensor:\n",
        "        mask = input_ids.ne(self.pad_token_id).int()\n",
        "        incremental_indices = (torch.cumsum(mask, dim=1).type_as(mask)) * mask\n",
        "        return incremental_indices.long() + self.pad_token_id\n",
        "\n",
        "    def forward(self, input_ids: torch.Tensor, token_type_ids: torch.Tensor | None = None):\n",
        "        if token_type_ids is None:\n",
        "            token_type_ids = torch.zeros_like(input_ids)\n",
        "\n",
        "        position_ids = self.create_position_ids_from_input_ids(input_ids)\n",
        "        inputs_embeds = self.word_embeddings(input_ids)\n",
        "        position_embeds = self.position_embeddings(position_ids)\n",
        "        token_type_embeds = self.token_type_embeddings(token_type_ids)\n",
        "        embeddings = inputs_embeds + position_embeds + token_type_embeds\n",
        "        embeddings = self.LayerNorm(embeddings)\n",
        "        embeddings = self.dropout(embeddings)\n",
        "        return embeddings\n",
        "\n",
        "\n",
        "class RobertaSelfAttention(nn.Module):\n",
        "    def __init__(self, config: RobertaConfig):\n",
        "        super().__init__()\n",
        "        self.num_heads = config.num_attention_heads\n",
        "        self.head_dim = config.hidden_size // config.num_attention_heads\n",
        "        self.all_head_size = self.num_heads * self.head_dim\n",
        "\n",
        "        self.query = nn.Linear(config.hidden_size, self.all_head_size)\n",
        "        self.key = nn.Linear(config.hidden_size, self.all_head_size)\n",
        "        self.value = nn.Linear(config.hidden_size, self.all_head_size)\n",
        "        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n",
        "\n",
        "    def transpose_for_scores(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        new_x_shape = x.size()[:-1] + (self.num_heads, self.head_dim)\n",
        "        x = x.view(*new_x_shape)\n",
        "        return x.permute(0, 2, 1, 3)\n",
        "\n",
        "    def forward(self, hidden_states: torch.Tensor, attention_mask: torch.Tensor | None = None):\n",
        "        query_layer = self.transpose_for_scores(self.query(hidden_states))\n",
        "        key_layer = self.transpose_for_scores(self.key(hidden_states))\n",
        "        value_layer = self.transpose_for_scores(self.value(hidden_states))\n",
        "\n",
        "        attn_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
        "        attn_scores = attn_scores / math.sqrt(self.head_dim)\n",
        "        if attention_mask is not None:\n",
        "            attn_scores = attn_scores + attention_mask\n",
        "\n",
        "        attn_probs = F.softmax(attn_scores, dim=-1)\n",
        "        attn_probs = self.dropout(attn_probs)\n",
        "        context_layer = torch.matmul(attn_probs, value_layer)\n",
        "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
        "        new_context_shape = context_layer.size()[:-2] + (self.all_head_size,)\n",
        "        context_layer = context_layer.view(*new_context_shape)\n",
        "        return context_layer, attn_probs\n",
        "\n",
        "\n",
        "class RobertaSelfOutput(nn.Module):\n",
        "    def __init__(self, config: RobertaConfig):\n",
        "        super().__init__()\n",
        "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
        "        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "\n",
        "    def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:\n",
        "        hidden_states = self.dense(hidden_states)\n",
        "        hidden_states = self.dropout(hidden_states)\n",
        "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
        "        return hidden_states\n",
        "\n",
        "\n",
        "class RobertaAttention(nn.Module):\n",
        "    def __init__(self, config: RobertaConfig):\n",
        "        super().__init__()\n",
        "        self.self = RobertaSelfAttention(config)\n",
        "        self.output = RobertaSelfOutput(config)\n",
        "\n",
        "    def forward(self, hidden_states: torch.Tensor, attention_mask: torch.Tensor | None = None):\n",
        "        self_outputs = self.self(hidden_states, attention_mask=attention_mask)\n",
        "        attn_output = self.output(self_outputs[0], hidden_states)\n",
        "        return attn_output, self_outputs[1]\n",
        "\n",
        "\n",
        "class RobertaIntermediate(nn.Module):\n",
        "    def __init__(self, config: RobertaConfig):\n",
        "        super().__init__()\n",
        "        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)\n",
        "        self.act = get_activation(config.hidden_act)\n",
        "\n",
        "    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n",
        "        return self.act(self.dense(hidden_states))\n",
        "\n",
        "\n",
        "class RobertaOutput(nn.Module):\n",
        "    def __init__(self, config: RobertaConfig):\n",
        "        super().__init__()\n",
        "        self.dense = nn.Linear(config.intermediate_size, config.hidden_size)\n",
        "        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "\n",
        "    def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:\n",
        "        hidden_states = self.dense(hidden_states)\n",
        "        hidden_states = self.dropout(hidden_states)\n",
        "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
        "        return hidden_states\n",
        "\n",
        "\n",
        "class RobertaLayer(nn.Module):\n",
        "    def __init__(self, config: RobertaConfig):\n",
        "        super().__init__()\n",
        "        self.attention = RobertaAttention(config)\n",
        "        self.intermediate = RobertaIntermediate(config)\n",
        "        self.output = RobertaOutput(config)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        hidden_states: torch.Tensor,\n",
        "        attention_mask: torch.Tensor | None = None,\n",
        "        return_attn_output: bool = False,\n",
        "    ):\n",
        "        attn_output, _ = self.attention(hidden_states, attention_mask=attention_mask)\n",
        "        intermediate_output = self.intermediate(attn_output)\n",
        "        layer_output = self.output(intermediate_output, attn_output)\n",
        "        if return_attn_output:\n",
        "            return layer_output, attn_output\n",
        "        return layer_output\n",
        "\n",
        "\n",
        "class RobertaEncoder(nn.Module):\n",
        "    def __init__(self, config: RobertaConfig):\n",
        "        super().__init__()\n",
        "        self.layer = nn.ModuleList(\n",
        "            [RobertaLayer(config) for _ in range(config.num_hidden_layers)]\n",
        "        )\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        hidden_states: torch.Tensor,\n",
        "        attention_mask: torch.Tensor | None = None,\n",
        "        return_attn_outputs: bool = False,\n",
        "        attn_output_layers: Iterable[int] | None = None,\n",
        "    ):\n",
        "        attn_outputs = {} if return_attn_outputs else None\n",
        "        target_layers = set(attn_output_layers or [])\n",
        "\n",
        "        for i, layer_module in enumerate(self.layer):\n",
        "            if return_attn_outputs and (i in target_layers):\n",
        "                hidden_states, attn_output = layer_module(\n",
        "                    hidden_states,\n",
        "                    attention_mask=attention_mask,\n",
        "                    return_attn_output=True,\n",
        "                )\n",
        "                attn_outputs[i] = attn_output\n",
        "            else:\n",
        "                hidden_states = layer_module(hidden_states, attention_mask=attention_mask)\n",
        "\n",
        "        if return_attn_outputs:\n",
        "            return hidden_states, attn_outputs\n",
        "        return hidden_states\n",
        "\n",
        "\n",
        "class RobertaModel(nn.Module):\n",
        "    def __init__(self, config: RobertaConfig):\n",
        "        super().__init__()\n",
        "        self.embeddings = RobertaEmbeddings(config)\n",
        "        self.encoder = RobertaEncoder(config)\n",
        "        self.config = config\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids: torch.Tensor,\n",
        "        attention_mask: torch.Tensor | None = None,\n",
        "        token_type_ids: torch.Tensor | None = None,\n",
        "        return_attn_outputs: bool = False,\n",
        "        attn_output_layers: Iterable[int] | None = None,\n",
        "    ):\n",
        "        if attention_mask is None:\n",
        "            attention_mask = torch.ones_like(input_ids)\n",
        "        extended_mask = (1.0 - attention_mask[:, None, None, :]) * -10000.0\n",
        "        embeddings = self.embeddings(input_ids, token_type_ids=token_type_ids)\n",
        "        encoder_out = self.encoder(\n",
        "            embeddings,\n",
        "            attention_mask=extended_mask,\n",
        "            return_attn_outputs=return_attn_outputs,\n",
        "            attn_output_layers=attn_output_layers,\n",
        "        )\n",
        "        return encoder_out\n",
        "\n",
        "\n",
        "class RobertaLMHead(nn.Module):\n",
        "    def __init__(self, config: RobertaConfig):\n",
        "        super().__init__()\n",
        "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
        "        self.layer_norm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
        "        self.decoder = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n",
        "        self.bias = nn.Parameter(torch.zeros(config.vocab_size))\n",
        "        self.decoder.bias = self.bias\n",
        "\n",
        "    def forward(self, features: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.dense(features)\n",
        "        x = F.gelu(x)\n",
        "        x = self.layer_norm(x)\n",
        "        x = self.decoder(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class RobertaForMaskedLM(nn.Module):\n",
        "    def __init__(self, config: RobertaConfig):\n",
        "        super().__init__()\n",
        "        self.roberta = RobertaModel(config)\n",
        "        self.lm_head = RobertaLMHead(config)\n",
        "        self.config = config\n",
        "\n",
        "    def tie_weights(self):\n",
        "        self.lm_head.decoder.weight = self.roberta.embeddings.word_embeddings.weight\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids: torch.Tensor,\n",
        "        attention_mask: torch.Tensor | None = None,\n",
        "        labels: torch.Tensor | None = None,\n",
        "        return_attn_outputs: bool = False,\n",
        "        attn_output_layers: Iterable[int] | None = None,\n",
        "    ):\n",
        "        encoder_out = self.roberta(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            return_attn_outputs=return_attn_outputs,\n",
        "            attn_output_layers=attn_output_layers,\n",
        "        )\n",
        "        if return_attn_outputs:\n",
        "            sequence_output, attn_outputs = encoder_out\n",
        "        else:\n",
        "            sequence_output = encoder_out\n",
        "            attn_outputs = None\n",
        "\n",
        "        logits = self.lm_head(sequence_output)\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            loss_fn = nn.CrossEntropyLoss(ignore_index=-100)\n",
        "            loss = loss_fn(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
        "\n",
        "        if return_attn_outputs:\n",
        "            return logits, loss, attn_outputs\n",
        "        return logits, loss\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## HF Loaders & Datasets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_state_dict_from_hf(model_name: str, local_only: bool = True):\n",
        "    snapshot_dir = snapshot_download(repo_id=model_name, local_files_only=local_only)\n",
        "    safetensors_path = Path(snapshot_dir) / \"model.safetensors\"\n",
        "    bin_path = Path(snapshot_dir) / \"pytorch_model.bin\"\n",
        "    if safetensors_path.exists():\n",
        "        from safetensors.torch import load_file\n",
        "\n",
        "        return load_file(str(safetensors_path))\n",
        "    return torch.load(str(bin_path), map_location=\"cpu\")\n",
        "\n",
        "\n",
        "def load_config_from_hf(model_name: str, local_only: bool = True) -> RobertaConfig:\n",
        "    snapshot_dir = snapshot_download(repo_id=model_name, local_files_only=local_only)\n",
        "    cfg_path = Path(snapshot_dir) / \"config.json\"\n",
        "    with open(cfg_path) as f:\n",
        "        cfg_dict = json.load(f)\n",
        "    return RobertaConfig(**cfg_dict)\n",
        "\n",
        "\n",
        "class MLMSmilesDataset(Dataset):\n",
        "    def __init__(self, path: Path, tokenizer: RobertaTokenizerFast, max_len: int = 128):\n",
        "        self.lines = [l.strip() for l in open(path) if l.strip()]\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.lines)\n",
        "\n",
        "    def __getitem__(self, idx: int):\n",
        "        enc = self.tokenizer(\n",
        "            self.lines[idx],\n",
        "            truncation=True,\n",
        "            padding=\"max_length\",\n",
        "            max_length=self.max_len,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "        return {k: v.squeeze(0) for k, v in enc.items()}\n",
        "\n",
        "\n",
        "class SmilesClassificationDataset(Dataset):\n",
        "    def __init__(self, df, tokenizer: RobertaTokenizerFast, label_cols, max_len: int = 128):\n",
        "        self.texts = df[\"text\"].tolist()\n",
        "        self.labels = df[label_cols].values\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx: int):\n",
        "        enc = self.tokenizer(\n",
        "            self.texts[idx],\n",
        "            truncation=True,\n",
        "            padding=\"max_length\",\n",
        "            max_length=self.max_len,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "        item = {k: v.squeeze(0) for k, v in enc.items()}\n",
        "        item[\"labels\"] = torch.tensor(self.labels[idx], dtype=torch.float)\n",
        "        return item\n",
        "\n",
        "\n",
        "class ActivationChunkDataset(IterableDataset):\n",
        "    def __init__(\n",
        "        self,\n",
        "        chunk_paths: list[Path],\n",
        "        batch_size: int,\n",
        "        shuffle: bool,\n",
        "        seed: int,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.chunk_paths = chunk_paths\n",
        "        self.batch_size = batch_size\n",
        "        self.shuffle = shuffle\n",
        "        self.seed = seed\n",
        "\n",
        "    def __iter__(self):\n",
        "        rng = random.Random(self.seed)\n",
        "        paths = list(self.chunk_paths)\n",
        "        if self.shuffle:\n",
        "            rng.shuffle(paths)\n",
        "        for path in paths:\n",
        "            acts = torch.load(path, map_location=\"cpu\")\n",
        "            if self.shuffle:\n",
        "                idx = torch.randperm(acts.shape[0])\n",
        "                acts = acts[idx]\n",
        "            for i in range(0, acts.shape[0], self.batch_size):\n",
        "                yield acts[i : i + self.batch_size]\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class SaeExperimentConfig:\n",
        "    model_name: str = \"seyonec/ChemBERTa-zinc-base-v1\"\n",
        "    local_only: bool = True\n",
        "    mlm_data_path: Path = (\n",
        "        _capstone_root()\n",
        "        / \"chemberta_repro_final/code/bert-loves-chemistry/chemberta/data/100k_rndm_zinc_drugs_clean.txt\"\n",
        "    )\n",
        "    max_len: int = 128\n",
        "    mlm_batch_size: int = 8\n",
        "\n",
        "    n_latents: int = 4096\n",
        "    topk: int = 32\n",
        "    sae_lr: float = 1e-4\n",
        "    sae_batch_size: int = 2048\n",
        "    sae_epochs: int = 2\n",
        "    l1_weight: float = 0.0\n",
        "    chunk_size: int = 20000\n",
        "    val_fraction: float = 0.05\n",
        "    seed: int = 42\n",
        "\n",
        "    layers: tuple[int, ...] = (0, 1, 2, 3, 4, 5)\n",
        "\n",
        "    runs_dir: Path = _capstone_root() / \"runs/sae\"\n",
        "    acts_dir: Path = _capstone_root() / \"runs/sae/acts\"\n",
        "    ckpt_dir: Path = _capstone_root() / \"runs/sae/checkpoints\"\n",
        "    log_path: Path = _capstone_root() / \"runs/sae/experiments.csv\"\n",
        "\n",
        "    downstream_tasks: tuple[str, ...] = (\"bbbp\", \"bace_classification\", \"clintox\")\n",
        "\n",
        "\n",
        "def prepare_mlm_loader(cfg: SaeExperimentConfig, tokenizer: RobertaTokenizerFast):\n",
        "    dataset = MLMSmilesDataset(cfg.mlm_data_path, tokenizer, max_len=cfg.max_len)\n",
        "    collator = DataCollatorForLanguageModeling(\n",
        "        tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
        "    )\n",
        "    return DataLoader(dataset, batch_size=cfg.mlm_batch_size, shuffle=True, collate_fn=collator)\n",
        "\n",
        "\n",
        "def build_mlm_model(cfg: SaeExperimentConfig, device: torch.device):\n",
        "    config = load_config_from_hf(cfg.model_name, local_only=cfg.local_only)\n",
        "    model = RobertaForMaskedLM(config)\n",
        "    model.tie_weights()\n",
        "    state_dict = load_state_dict_from_hf(cfg.model_name, local_only=cfg.local_only)\n",
        "    model.load_state_dict(state_dict, strict=False)\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    return model, config\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Activation Extraction\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "def save_meta(path: Path, meta: dict) -> None:\n",
        "    path.parent.mkdir(parents=True, exist_ok=True)\n",
        "    with open(path, \"w\") as f:\n",
        "        json.dump(meta, f, indent=2)\n",
        "\n",
        "\n",
        "def _write_chunk(layer_dir: Path, chunk_idx: int, tensor: torch.Tensor) -> Path:\n",
        "    layer_dir.mkdir(parents=True, exist_ok=True)\n",
        "    path = layer_dir / f\"chunk_{chunk_idx:05d}.pt\"\n",
        "    torch.save(tensor, path)\n",
        "    return path\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def extract_attn_activations(\n",
        "    cfg: SaeExperimentConfig, model: RobertaForMaskedLM, loader: DataLoader, device: torch.device\n",
        ") -> None:\n",
        "    model.eval()\n",
        "    for layer in cfg.layers:\n",
        "        layer_dir = cfg.acts_dir / f\"layer_{layer}\"\n",
        "        chunk_idx = 0\n",
        "        buffered = []\n",
        "        buffered_tokens = 0\n",
        "        total_tokens = 0\n",
        "        chunk_paths = []\n",
        "\n",
        "        for batch in loader:\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            _, attn_outputs = model.roberta(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                return_attn_outputs=True,\n",
        "                attn_output_layers={layer},\n",
        "            )\n",
        "            attn = attn_outputs[layer]\n",
        "            flat = attn[attention_mask.bool()].detach().cpu().to(torch.float16)\n",
        "            buffered.append(flat)\n",
        "            buffered_tokens += flat.shape[0]\n",
        "            total_tokens += flat.shape[0]\n",
        "\n",
        "            if buffered_tokens >= cfg.chunk_size:\n",
        "                chunk = torch.cat(buffered, dim=0)\n",
        "                chunk_paths.append(_write_chunk(layer_dir, chunk_idx, chunk))\n",
        "                chunk_idx += 1\n",
        "                buffered = []\n",
        "                buffered_tokens = 0\n",
        "\n",
        "        if buffered:\n",
        "            chunk = torch.cat(buffered, dim=0)\n",
        "            chunk_paths.append(_write_chunk(layer_dir, chunk_idx, chunk))\n",
        "\n",
        "        meta = {\n",
        "            \"layer\": layer,\n",
        "            \"d_model\": attn.shape[-1],\n",
        "            \"num_tokens\": total_tokens,\n",
        "            \"num_chunks\": len(chunk_paths),\n",
        "            \"chunk_size\": cfg.chunk_size,\n",
        "            \"dtype\": \"float16\",\n",
        "            \"model_name\": cfg.model_name,\n",
        "            \"mlm_data_path\": str(cfg.mlm_data_path),\n",
        "        }\n",
        "        save_meta(layer_dir / \"meta.json\", meta)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## SAE Training (resume)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "def _list_chunks(layer_dir: Path) -> list[Path]:\n",
        "    return sorted(layer_dir.glob(\"chunk_*.pt\"))\n",
        "\n",
        "\n",
        "def _latest_checkpoint(path: Path) -> Path | None:\n",
        "    if not path.exists():\n",
        "        return None\n",
        "    latest = path / \"latest.pt\"\n",
        "    if latest.exists():\n",
        "        return latest\n",
        "    ckpts = sorted(path.glob(\"checkpoint_step_*.pt\"))\n",
        "    return ckpts[-1] if ckpts else None\n",
        "\n",
        "\n",
        "def _save_checkpoint(path: Path, state: dict) -> None:\n",
        "    path.parent.mkdir(parents=True, exist_ok=True)\n",
        "    torch.save(state, path)\n",
        "\n",
        "\n",
        "def train_sae_for_layer(\n",
        "    cfg: SaeExperimentConfig, layer: int, device: torch.device, resume: bool = True\n",
        "):\n",
        "    layer_dir = cfg.acts_dir / f\"layer_{layer}\"\n",
        "    chunk_paths = _list_chunks(layer_dir)\n",
        "    if not chunk_paths:\n",
        "        raise FileNotFoundError(f\"No activation chunks found in {layer_dir}\")\n",
        "\n",
        "    train_cut = max(1, int(len(chunk_paths) * (1 - cfg.val_fraction)))\n",
        "    train_paths = chunk_paths[:train_cut]\n",
        "    val_paths = chunk_paths[train_cut:] or train_paths[-1:]\n",
        "\n",
        "    train_data = ActivationChunkDataset(\n",
        "        train_paths, batch_size=cfg.sae_batch_size, shuffle=True, seed=cfg.seed\n",
        "    )\n",
        "    val_data = ActivationChunkDataset(\n",
        "        val_paths, batch_size=cfg.sae_batch_size, shuffle=False, seed=cfg.seed\n",
        "    )\n",
        "\n",
        "    d_model = torch.load(chunk_paths[0], map_location=\"cpu\").shape[1]\n",
        "    ae = Autoencoder(\n",
        "        n_latents=cfg.n_latents,\n",
        "        n_inputs=d_model,\n",
        "        activation=TopK(cfg.topk),\n",
        "        normalize=True,\n",
        "    ).to(device)\n",
        "    optimizer = torch.optim.AdamW(ae.parameters(), lr=cfg.sae_lr)\n",
        "\n",
        "    start_epoch = 0\n",
        "    global_step = 0\n",
        "    ckpt_dir = cfg.ckpt_dir / f\"layer_{layer}\"\n",
        "    if resume:\n",
        "        latest = _latest_checkpoint(ckpt_dir)\n",
        "        if latest is not None:\n",
        "            state = torch.load(latest, map_location=device)\n",
        "            ae.load_state_dict(state[\"model\"])\n",
        "            optimizer.load_state_dict(state[\"optimizer\"])\n",
        "            start_epoch = state[\"epoch\"] + 1\n",
        "            global_step = state[\"step\"]\n",
        "\n",
        "    for epoch in range(start_epoch, cfg.sae_epochs):\n",
        "        ae.train()\n",
        "        train_loss = 0.0\n",
        "        num_batches = 0\n",
        "        for batch in train_data:\n",
        "            batch = batch.to(device).float()\n",
        "            _, latents, recons = ae(batch)\n",
        "            loss = autoencoder_loss(recons, batch, latents, cfg.l1_weight)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "            train_loss += loss.item()\n",
        "            num_batches += 1\n",
        "            global_step += 1\n",
        "            if global_step % 1000 == 0:\n",
        "                _save_checkpoint(\n",
        "                    ckpt_dir / f\"checkpoint_step_{global_step}.pt\",\n",
        "                    {\n",
        "                        \"model\": ae.state_dict(),\n",
        "                        \"optimizer\": optimizer.state_dict(),\n",
        "                        \"epoch\": epoch,\n",
        "                        \"step\": global_step,\n",
        "                    },\n",
        "                )\n",
        "                _save_checkpoint(\n",
        "                    ckpt_dir / \"latest.pt\",\n",
        "                    {\n",
        "                        \"model\": ae.state_dict(),\n",
        "                        \"optimizer\": optimizer.state_dict(),\n",
        "                        \"epoch\": epoch,\n",
        "                        \"step\": global_step,\n",
        "                    },\n",
        "                )\n",
        "\n",
        "        ae.eval()\n",
        "        val_loss = 0.0\n",
        "        val_batches = 0\n",
        "        with torch.no_grad():\n",
        "            for batch in val_data:\n",
        "                batch = batch.to(device).float()\n",
        "                _, latents, recons = ae(batch)\n",
        "                loss = autoencoder_loss(recons, batch, latents, cfg.l1_weight)\n",
        "                val_loss += loss.item()\n",
        "                val_batches += 1\n",
        "\n",
        "        _save_checkpoint(\n",
        "            ckpt_dir / f\"checkpoint_step_{global_step}.pt\",\n",
        "            {\n",
        "                \"model\": ae.state_dict(),\n",
        "                \"optimizer\": optimizer.state_dict(),\n",
        "                \"epoch\": epoch,\n",
        "                \"step\": global_step,\n",
        "            },\n",
        "        )\n",
        "        _save_checkpoint(\n",
        "            ckpt_dir / \"latest.pt\",\n",
        "            {\n",
        "                \"model\": ae.state_dict(),\n",
        "                \"optimizer\": optimizer.state_dict(),\n",
        "                \"epoch\": epoch,\n",
        "                \"step\": global_step,\n",
        "            },\n",
        "        )\n",
        "\n",
        "        print(\n",
        "            f\"[layer {layer}] epoch {epoch+1}/{cfg.sae_epochs} \"\n",
        "            f\"train_loss={train_loss/max(1,num_batches):.4f} \"\n",
        "            f\"val_loss={val_loss/max(1,val_batches):.4f}\"\n",
        "        )\n",
        "\n",
        "    return ae\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Downstream Evaluation & CSV Logging\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def compute_latent_features(\n",
        "    model: RobertaForMaskedLM,\n",
        "    ae: Autoencoder,\n",
        "    dataloader: DataLoader,\n",
        "    layer: int,\n",
        "    device: torch.device,\n",
        "):\n",
        "    model.eval()\n",
        "    ae.eval()\n",
        "    feats = []\n",
        "    labels = []\n",
        "    for batch in dataloader:\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        label = batch[\"labels\"].cpu()\n",
        "\n",
        "        _, attn_outputs = model.roberta(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            return_attn_outputs=True,\n",
        "            attn_output_layers={layer},\n",
        "        )\n",
        "        attn = attn_outputs[layer]\n",
        "        flat = attn.reshape(-1, attn.shape[-1])\n",
        "        latents, _ = ae.encode(flat)\n",
        "        latents = latents.reshape(attn.shape[0], attn.shape[1], -1)\n",
        "        mask = attention_mask.unsqueeze(-1).float()\n",
        "        pooled = (latents * mask).sum(dim=1) / mask.sum(dim=1).clamp(min=1.0)\n",
        "        feats.append(pooled.cpu())\n",
        "        labels.append(label)\n",
        "    return torch.cat(feats, dim=0).numpy(), torch.cat(labels, dim=0).numpy()\n",
        "\n",
        "\n",
        "def train_linear_probe(X_train: np.ndarray, y_train: np.ndarray):\n",
        "    clf = LogisticRegression(max_iter=1000)\n",
        "    clf.fit(X_train, y_train)\n",
        "    return clf\n",
        "\n",
        "\n",
        "def eval_roc_auc(clf: LogisticRegression, X: np.ndarray, y: np.ndarray) -> float:\n",
        "    probs = clf.predict_proba(X)[:, 1]\n",
        "    return roc_auc_score(y, probs)\n",
        "\n",
        "\n",
        "def append_csv_row(path: Path, row: dict) -> None:\n",
        "    path.parent.mkdir(parents=True, exist_ok=True)\n",
        "    write_header = not path.exists()\n",
        "    with open(path, \"a\", newline=\"\") as f:\n",
        "        writer = csv.DictWriter(f, fieldnames=list(row.keys()))\n",
        "        if write_header:\n",
        "            writer.writeheader()\n",
        "        writer.writerow(row)\n",
        "\n",
        "\n",
        "def evaluate_downstream(\n",
        "    cfg: SaeExperimentConfig,\n",
        "    model: RobertaForMaskedLM,\n",
        "    ae: Autoencoder,\n",
        "    tokenizer: RobertaTokenizerFast,\n",
        "    layer: int,\n",
        "    device: torch.device,\n",
        "):\n",
        "    for task in cfg.downstream_tasks:\n",
        "        label_cols, (train_df, valid_df, test_df), _ = load_molnet_dataset(task)\n",
        "        train_dataset = SmilesClassificationDataset(\n",
        "            train_df, tokenizer, label_cols, max_len=cfg.max_len\n",
        "        )\n",
        "        test_dataset = SmilesClassificationDataset(\n",
        "            test_df, tokenizer, label_cols, max_len=cfg.max_len\n",
        "        )\n",
        "        train_loader = DataLoader(train_dataset, batch_size=32, shuffle=False)\n",
        "        test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "        X_train, y_train = compute_latent_features(\n",
        "            model, ae, train_loader, layer, device\n",
        "        )\n",
        "        X_test, y_test = compute_latent_features(\n",
        "            model, ae, test_loader, layer, device\n",
        "        )\n",
        "\n",
        "        if y_train.ndim > 1:\n",
        "            y_train = y_train[:, 0]\n",
        "            y_test = y_test[:, 0]\n",
        "\n",
        "        mask_train = y_train != -1\n",
        "        mask_test = y_test != -1\n",
        "        clf = train_linear_probe(X_train[mask_train], y_train[mask_train])\n",
        "        roc = eval_roc_auc(clf, X_test[mask_test], y_test[mask_test])\n",
        "\n",
        "        row = {\n",
        "            \"run_id\": f\"{int(time.time())}\",\n",
        "            \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
        "            \"model_name\": cfg.model_name,\n",
        "            \"layer\": layer,\n",
        "            \"attn_source\": \"attn_output\",\n",
        "            \"sae_apply_mode\": \"model_mod\",\n",
        "            \"sae_training_mode\": \"pretrain_only\",\n",
        "            \"backbone_frozen\": True,\n",
        "            \"sae_type\": \"TopK\",\n",
        "            \"n_latents\": cfg.n_latents,\n",
        "            \"k\": cfg.topk,\n",
        "            \"l1_weight\": cfg.l1_weight,\n",
        "            \"lr\": cfg.sae_lr,\n",
        "            \"batch_size\": cfg.sae_batch_size,\n",
        "            \"epochs\": cfg.sae_epochs,\n",
        "            \"seed\": cfg.seed,\n",
        "            \"mlm_data_path\": str(cfg.mlm_data_path),\n",
        "            \"downstream_task\": task,\n",
        "            \"downstream_method\": \"latent_probe\",\n",
        "            \"pooling\": \"mean\",\n",
        "            \"probe_model\": \"linear\",\n",
        "            \"roc_auc\": roc,\n",
        "            \"notes\": \"\",\n",
        "        }\n",
        "        append_csv_row(cfg.log_path, row)\n",
        "        print(f\"[layer {layer}] {task} ROC-AUC={roc:.4f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_872905/1615843932.py:9: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  return torch.load(str(bin_path), map_location=\"cpu\")\n",
            "/tmp/ipykernel_872905/3328615991.py:39: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  d_model = torch.load(chunk_paths[0], map_location=\"cpu\").shape[1]\n",
            "/tmp/ipykernel_872905/3328615991.py:54: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(latest, map_location=device)\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "Error(s) in loading state_dict for Autoencoder:\n\tUnexpected key(s) in state_dict: \"activation\", \"activation_state_dict\", \"activation.k\", \"activation.postact_fn\". ",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[7], line 19\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     18\u001b[0m     cfg \u001b[38;5;241m=\u001b[39m SaeExperimentConfig()\n\u001b[0;32m---> 19\u001b[0m     \u001b[43mrun_all\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresume\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[7], line 13\u001b[0m, in \u001b[0;36mrun_all\u001b[0;34m(cfg, resume)\u001b[0m\n\u001b[1;32m     10\u001b[0m extract_attn_activations(cfg, model, mlm_loader, device)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m cfg\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m---> 13\u001b[0m     ae \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_sae_for_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresume\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m     evaluate_downstream(cfg, model, ae, tokenizer, layer, device)\n",
            "Cell \u001b[0;32mIn[5], line 55\u001b[0m, in \u001b[0;36mtrain_sae_for_layer\u001b[0;34m(cfg, layer, device, resume)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m latest \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     state \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(latest, map_location\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m---> 55\u001b[0m     \u001b[43mae\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mload_state_dict(state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moptimizer\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     57\u001b[0m     start_epoch \u001b[38;5;241m=\u001b[39m state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
            "File \u001b[0;32m~/micromamba/envs/chemberta-repro/lib/python3.10/site-packages/torch/nn/modules/module.py:2584\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2576\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2577\u001b[0m             \u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m   2578\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2579\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)\n\u001b[1;32m   2580\u001b[0m             ),\n\u001b[1;32m   2581\u001b[0m         )\n\u001b[1;32m   2583\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2584\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   2585\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2586\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)\n\u001b[1;32m   2587\u001b[0m         )\n\u001b[1;32m   2588\u001b[0m     )\n\u001b[1;32m   2589\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for Autoencoder:\n\tUnexpected key(s) in state_dict: \"activation\", \"activation_state_dict\", \"activation.k\", \"activation.postact_fn\". "
          ]
        }
      ],
      "source": [
        "def run_all(cfg: SaeExperimentConfig, resume: bool = True):\n",
        "    set_seed(cfg.seed)\n",
        "    device = torch.device(\"cuda:2\" if torch.cuda.is_available() else \"cpu\")\n",
        "    tokenizer = RobertaTokenizerFast.from_pretrained(\n",
        "        cfg.model_name, local_files_only=cfg.local_only\n",
        "    )\n",
        "    model, _ = build_mlm_model(cfg, device)\n",
        "\n",
        "    mlm_loader = prepare_mlm_loader(cfg, tokenizer)\n",
        "    extract_attn_activations(cfg, model, mlm_loader, device)\n",
        "\n",
        "    for layer in cfg.layers:\n",
        "        ae = train_sae_for_layer(cfg, layer, device, resume=resume)\n",
        "        evaluate_downstream(cfg, model, ae, tokenizer, layer, device)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    cfg = SaeExperimentConfig()\n",
        "    run_all(cfg, resume=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e3ec5667",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "chemberta-repro",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
